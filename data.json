[
    {
        "model": "BERT",
        "paper": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "url": "https://arxiv.org/abs/1810.04805",
        "pdf_path": "data/bert_paper.pdf",
        "year": 2018,
        "tasks": ["GLUE", "SQuAD"],
        "datasets": ["Wikipedia", "BookCorpus"],
        "derived_from": "Transformer"
    },
    {
        "model": "DistilBERT",
        "paper": "DistilBERT: A Distilled Version of BERT: smaller, faster, cheaper and lighter",
        "url": "https://arxiv.org/abs/1910.01108",
        "pdf_path": "data/distilbert_paper.pdf",
        "year": 2019,
        "tasks": ["GLUE"],
        "datasets": ["Wikipedia", "BookCorpus"],
        "derived_from": "BERT"
    },
    {
        "model": "TinyBERT",
        "paper": "TinyBERT: Distilling BERT for Natural Language Understanding",
        "url": "https://arxiv.org/abs/1909.10351",
        "pdf_path": "data/tinybert_paper.pdf",
        "year": 2019,
        "tasks": ["GLUE", "MNLI"],
        "datasets": ["Wikipedia", "BookCorpus"],
        "derived_from": "BERT"
    },
    {
        "model": "MobileBERT",
        "paper": "MobileBERT: A Compact Task-Agnostic BERT for Resource-Limited Devices",
        "url": "https://arxiv.org/abs/2004.02984",
        "pdf_path": "data/mobilebert_paper.pdf",
        "year": 2020,
        "tasks": ["GLUE", "SQuAD"],
        "datasets": ["Wikipedia", "BookCorpus"],
        "derived_from": "BERT_LARGE"
    },
    {
        "model": "GPT",
        "paper": "Improving Language Understanding by Generative Pre-Training",
        "url": "https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf",
        "pdf_path": "data/gpt_paper.pdf",
        "year": 2018,
        "tasks": ["Text Classification", "Natural Language Inference", "Question Answering", "Semantic Similarity"],
        "datasets": ["BooksCorpus"],
        "derived_from": "Transformer"
    }
]